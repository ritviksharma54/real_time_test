<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Real-Time Voice AI with Gemini</title>
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/socket.io/4.7.2/socket.io.js"></script>
  <style>
    body {
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
        line-height: 1.6;
        background-color: #f4f7f6;
        color: #333;
    }
    .visualizer {
      width: 100%;
      height: 100px; 
      background-color: #e9ecef; 
      border: 1px solid #ced4da; 
      border-radius: 8px;
      margin-bottom: 20px;
      box-shadow: inset 0 1px 3px rgba(0,0,0,0.1);
    }

    .status-indicator {
      width: 12px;
      height: 12px;
      border-radius: 50%;
      display: inline-block;
      margin-right: 8px;
      vertical-align: middle; 
    }

    .status-connected { background-color: #28a745; }
    .status-listening { background-color: #ffc107; }
    .status-processing { background-color: #0dcaf0; }
    .status-speaking { background-color: #dc3545; }
    .status-idle { background-color: #6c757d; }
    .status-error { background-color: #dc3545; }
    .status-vad-active { background-color: #007bff; }

    .message-bubble {
      max-width: 75%; 
      margin-bottom: 12px; 
      padding: 10px 15px; 
      border-radius: 20px; 
      word-wrap: break-word;
      position: relative;
      box-shadow: 0 2px 5px rgba(0,0,0,0.08);
      line-height: 1.5;
    }

    .message-user {
      background-color: #007bff;
      color: white;
      margin-left: auto;
      border-bottom-right-radius: 5px; 
    }

    .message-assistant {
      background-color: #f8f9fa; 
      color: #343a40; 
      margin-right: auto;
      border: 1px solid #e9ecef;
      border-bottom-left-radius: 5px; 
    }

    .chat-container {
      height: 350px; 
      overflow-y: auto;
      border: 1px solid #dee2e6;
      border-radius: 8px;
      padding: 20px; 
      background-color: #ffffff; 
      margin-bottom: 20px; 
    }

    .controls {
      text-align: center;
      margin: 20px 0;
    }

    .btn-voice {
      width: 80px;
      height: 80px;
      border-radius: 50%;
      border: none;
      font-size: 28px;
      transition: all 0.2s ease-in-out;
      position: relative;
      overflow: hidden;
      background-color: #6c757d;
      color: white;
      box-shadow: 0 4px 8px rgba(0,0,0,0.15);
    }
    .btn-voice:hover {
        transform: translateY(-2px);
        box-shadow: 0 6px 12px rgba(0,0,0,0.2);
    }
    .btn-voice:active {
      transform: scale(0.95) translateY(1px);
      box-shadow: 0 2px 4px rgba(0,0,0,0.15);
    }

    .btn-voice.vad-listening-mode { background-color: #007bff; }
    .btn-voice.user-speaking { background-color: #ffc107; color: #212529; animation: pulse-yellow 1.2s infinite; }
    .btn-voice.ai-processing { background-color: #0dcaf0; color: #212529; animation: spin 1.5s linear infinite; }
    .btn-voice.ai-speaking { background-color: #e83e8c; }


    @keyframes pulse-yellow {
      0% { transform: scale(1); box-shadow: 0 0 0 0 rgba(255, 193, 7, 0.7), 0 4px 8px rgba(0,0,0,0.15); }
      70% { transform: scale(1.03); box-shadow: 0 0 0 8px rgba(255, 193, 7, 0), 0 6px 12px rgba(0,0,0,0.2); }
      100% { transform: scale(1); box-shadow: 0 0 0 0 rgba(255, 193, 7, 0), 0 4px 8px rgba(0,0,0,0.15); }
    }

    @keyframes spin {
      0% { transform: rotate(0deg); }
      100% { transform: rotate(360deg); }
    }

    .connection-status {
      padding: 6px 12px; 
      border-radius: 15px; 
      font-size: 0.85rem; 
      font-weight: 500;
    }
    .connection-connected { background-color: #d1e7dd; color: #0f5132; border: 1px solid #badbcc;}
    .connection-disconnected { background-color: #f8d7da; color: #842029; border: 1px solid #f5c2c7;}

    .audio-controls-card, .chat-card, .settings-card { /* Using classes for cards */
        background-color: #ffffff;
        border: 1px solid #dee2e6;
        border-radius: 0.5rem; /* Bootstrap's default card radius */
        box-shadow: 0 0.125rem 0.25rem rgba(0, 0, 0, 0.075);
        margin-bottom: 1.5rem;
    }
    .card-header {
        background-color: #f8f9fa;
        border-bottom: 1px solid #dee2e6;
        font-weight: 500;
    }

    .transcription-display {
      background-color: #e9f5ff; /* Lighter blue */
      border: 1px solid #b8d6f0;
      border-radius: 8px;
      padding: 12px; /* Adjusted padding */
      margin: 15px 0;
      font-style: italic;
      min-height: 40px; /* Adjusted height */
      font-size: 0.9rem; /* Smaller font */
      color: #334d6e;
    }
    .detected-lang {
        font-weight: bold;
        color: #0056b3;
        margin-right: 5px;
    }

    .loading-dots:after {
      content: '';
      animation: dots 1.5s infinite steps(1, end); /* Use steps for better dot animation */
      display: inline-block;
      vertical-align: bottom;
      width: 1em; /* Adjust width to control number of dots shown */
    }

    @keyframes dots {
      0%, 20% { content: '.'; }
      40% { content: '..'; }
      60%, 100% { content: '...'; }
    }
    .btn-sm {
        padding: 0.25rem 0.5rem;
        font-size: 0.8rem;
    }
    .container { max-width: 1140px; } /* Max width for larger screens */
  </style>
</head>
<body class="p-3">
  <div class="container">
    <header class="text-center py-4 mb-4 border-bottom">
        <h1 class="h2">ü§ñ Real-Time Voice AI with Gemini</h1>
        <p class="text-muted">Powered by Gemini, gTTS & Whisper</p>
    </header>

    <div class="d-flex justify-content-between align-items-center mb-4">
      <div>
        <span id="connectionStatus" class="connection-status connection-disconnected">
          ‚ö†Ô∏è Connecting...
        </span>
      </div>
      <div>
        <button id="testGemini" class="btn btn-outline-primary btn-sm">
          Test Gemini API
        </button>
      </div>
    </div>

    <div class="card audio-controls-card mb-4">
        <div class="card-header">
            <span id="statusIndicator" class="status-indicator status-idle"></span>
            Status: <span id="statusText">Ready</span>
        </div>
        <div class="card-body">
            <p class="card-text text-muted small" id="statusDetail">Click the microphone to start listening</p>
        </div>
    </div>


    <div class="row">
      <div class="col-lg-6">
        <!-- Input Language Selector -->
        <div class="settings-card mb-3">
            <div class="card-header">
                <h6 class="mb-0">üó£Ô∏è Input Language</h6>
            </div>
            <div class="card-body">
                <label for="inputLanguageSelect" class="form-label small">I will speak in:</label>
                <select id="inputLanguageSelect" class="form-select form-select-sm">
                    <option value="en" selected>English (en)</option>
                    <option value="hi">Hindi (hi)</option>
                </select>
                <small class="form-text text-muted mt-2 d-block">
                    Select your primary speaking language for best results.
                </small>
            </div>
        </div>

        <div class="audio-controls-card">
          <div class="card-header">üéôÔ∏è Voice Controls</div>
          <div class="card-body">
            <div class="controls">
              <button id="voiceBtn" class="btn btn-voice" title="Click to toggle listening mode">
                üé§
              </button>
              <div class="mt-3">
                <small class="text-muted" id="voiceBtnHelpText">Click to enable hands-free mode</small>
              </div>
            </div>

            <div class="transcription-display" id="transcriptionDisplay">
              Your speech will appear here...
            </div>

            <div class="text-center mt-3">
              <button id="clearChat" class="btn btn-outline-danger btn-sm me-2">
                üóëÔ∏è Clear Chat
              </button>
              <button id="stopSpeaking" class="btn btn-outline-warning btn-sm">
                üîá Stop AI Speaking
              </button>
            </div>
          </div>
        </div>

        <div class="audio-controls-card mt-4">
            <div class="card-header">üéµ Audio Input Visualizer</div>
            <div class="card-body">
                <canvas id="audioVisualizer" class="visualizer"></canvas>
            </div>
        </div>
      </div>

      <div class="col-lg-6">
        <div class="chat-card">
          <div class="card-header d-flex justify-content-between align-items-center">
            <h5 class="mb-0 h6">üí¨ Conversation</h5>
          </div>
          <div class="chat-container" id="chatContainer">
            <div class="message-bubble message-assistant">
                Hello! I'm your AI assistant. Select your speaking language, then enable hands-free mode by clicking the microphone. I'll listen and respond!
            </div>
          </div>
        </div>

        <div class="settings-card mt-4">
          <div class="card-header">
            <h6 class="mb-0">üîß Voice Settings (Browser TTS Fallback)</h6>
          </div>
          <div class="card-body">
            <div class="row">
              <div class="col-md-6 mb-2 mb-md-0">
                <label for="voiceSelect" class="form-label small">Preferred Voice:</label>
                <select id="voiceSelect" class="form-select form-select-sm">
                  <option value="">Browser Default</option>
                </select>
              </div>
              <div class="col-md-3 mb-2 mb-md-0">
                <label for="speedRange" class="form-label small">Speed: <span id="speedValue">1.0</span></label>
                <input type="range" class="form-range" id="speedRange" min="0.5" max="2" step="0.1" value="1">
              </div>
              <div class="col-md-3">
                <label for="pitchRange" class="form-label small">Pitch: <span id="pitchValue">1.0</span></label>
                <input type="range" class="form-range" id="pitchRange" min="0.5" max="2" step="0.1" value="1">
              </div>
            </div>
            <small class="form-text text-muted mt-2 d-block">Note: Server-side TTS (gTTS) will be used primarily. These settings apply if browser TTS is used as a fallback.</small>
          </div>
        </div>
      </div>
    </div>
  </div>

  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
  <script>
    class VoiceAI {
      constructor() {
        this.socket = io({
            reconnectionAttempts: 5,
            reconnectionDelay: 3000
        });
        this.mediaRecorder = null;
        this.audioChunks = [];
        this.localStream = null;
        this.audioContext = null;
        this.analyser = null;
        this.sourceNode = null;
        this.animationId = null;

        this.isCapturingAudio = false;
        this.isProcessingAI = false;
        this.vadEnabled = false;
        this.userIsSpeakingVAD = false;
        this.isAISpeechCoolDown = false;
        this.currentServerAudio = null; // Stores the current <audio> element from server
        this.currentUtterance = null;  // Stores the current SpeechSynthesisUtterance

        this.synthesis = window.speechSynthesis;


        // VAD parameters
        this.VAD_ENERGY_THRESHOLD = 0.01;
        this.VAD_SILENCE_DURATION_MS = 1200;
        this.VAD_MIN_SPEECH_DURATION_MS = 700;
        this.AI_SPEECH_COOLDOWN_MS = 700;
        this.cooldownTimeoutId = null; // For managing cooldown timeout

        // VAD state variables
        this.silenceTimeoutId = null;
        this.speechStartTime = null;

        this.initElements();
        this.initSocketEvents();
        this.initSpeechSynthesis();
        this.bindEvents();
        this.updateStatus('idle', 'Ready', 'Click the microphone to start listening.');
        this.log("VoiceAI Initialized");
      }

      log(message, style = "color: blue;") {
        console.log(`%c[VoiceAI] ${message}`, style);
      }

      error(message, errorObj = null) {
        console.error(`%c[VoiceAI ERROR] ${message}`, "color: red; font-weight: bold;", errorObj || "");
      }

      initElements() {
        this.voiceBtn = document.getElementById('voiceBtn');
        this.voiceBtnHelpText = document.getElementById('voiceBtnHelpText');
        this.statusIndicator = document.getElementById('statusIndicator');
        this.statusText = document.getElementById('statusText');
        this.statusDetail = document.getElementById('statusDetail');
        this.chatContainer = document.getElementById('chatContainer');
        this.clearChat = document.getElementById('clearChat');
        this.stopSpeaking = document.getElementById('stopSpeaking');
        this.connectionStatus = document.getElementById('connectionStatus');
        this.testGemini = document.getElementById('testGemini');
        this.transcriptionDisplay = document.getElementById('transcriptionDisplay');
        this.voiceSelect = document.getElementById('voiceSelect');
        this.speedRange = document.getElementById('speedRange');
        this.pitchRange = document.getElementById('pitchRange');
        this.speedValue = document.getElementById('speedValue');
        this.pitchValue = document.getElementById('pitchValue');
        this.audioVisualizer = document.getElementById('audioVisualizer');
        this.inputLanguageSelect = document.getElementById('inputLanguageSelect');
        this.log("Elements Initialized");
      }

      initSocketEvents() {
        this.socket.on('connect', () => {
          this.log('‚úÖ Connected to server');
          this.connectionStatus.textContent = '‚úÖ Connected';
          this.connectionStatus.className = 'connection-status connection-connected';
          this.requestMicrophoneAccess();
        });

        this.socket.on('disconnect', (reason) => {
          this.log(`‚ùå Disconnected from server: ${reason}`, "color: orange;");
          this.connectionStatus.textContent = '‚ùå Disconnected';
          this.connectionStatus.className = 'connection-status connection-disconnected';
          this.stopVADAndVisualization();
        });

        this.socket.on('connect_error', (err) => {
          this.error('Connection Error', err);
          this.connectionStatus.textContent = '‚ö†Ô∏è Connection Failed';
          this.connectionStatus.className = 'connection-status connection-disconnected';
        });

        this.socket.on('audio_response', (data) => {
          this.log('ü§ñ Received AI response');
          this.handleAudioResponse(data);
        });

        this.socket.on('conversation_cleared', () => {
          this.chatContainer.innerHTML = '';
          this.addMessage('Chat cleared. How can I assist you next?', 'assistant');
          this.log("Conversation cleared by server.");
        });

        this.log("Socket Events Initialized");
      }

      initSpeechSynthesis() {
        const populateVoices = () => {
          try {
            const voices = this.synthesis.getVoices();
            if (!voices || voices.length === 0) {
                if (this.synthesis.onvoiceschanged === undefined) {
                    this.log("TTS voices not immediately available and onvoiceschanged not supported. Retrying voice population.");
                    setTimeout(populateVoices, 500);
                }
                return;
            }
            this.voiceSelect.innerHTML = '<option value="">Browser Default (Fallback)</option>';
            voices.forEach((voice, index) => {
              const option = document.createElement('option');
              option.value = index;
              option.textContent = `${voice.name} (${voice.lang})`;
              this.voiceSelect.appendChild(option);
            });
            this.log(`Populated ${voices.length} TTS voices.`);
          } catch (e) {
            this.error("Error populating TTS voices", e);
          }
        };

        if (this.synthesis.getVoices().length > 0) {
          populateVoices();
        } else if (typeof this.synthesis.onvoiceschanged !== 'undefined') {
          this.synthesis.onvoiceschanged = populateVoices;
        } else {
            this.log("onvoiceschanged not supported by this browser for TTS. Attempting to populate voices directly.");
            setTimeout(populateVoices, 1000); // Attempt to populate after a delay
        }
        this.log("Speech Synthesis Initialized");
      }

      async requestMicrophoneAccess() {
        this.log("Requesting microphone access...");
        if (this.localStream && this.localStream.active) {
          this.log("Microphone already active. Ensuring audio pipeline is running.");
          if (!this.audioContext || this.audioContext.state !== 'running') {
            this.initAudioPipeline();
          }
          return;
        }
        try {
          this.localStream = await navigator.mediaDevices.getUserMedia({
            audio: {
              echoCancellation: true,
              noiseSuppression: true,
              autoGainControl: true,
              sampleRate: 44100 // Common sample rate
            }
          });
          this.log('üé§ Microphone access granted.');
          this.initAudioPipeline();
        } catch (err) {
          this.error('Error accessing microphone', err);
          this.updateStatus('error', 'Mic Error', 'Could not access microphone. Please grant permission and refresh.');
          alert('Microphone access denied. Please grant permission in your browser settings and refresh the page.');
          this.localStream = null;
        }
      }

      initAudioPipeline() {
        if (!this.localStream || !this.localStream.active) {
          this.error("Cannot init audio pipeline: localStream is not available or inactive.");
          return false; // Indicate failure
        }
        if (this.audioContext && this.audioContext.state === 'running') {
          this.log("Audio pipeline already initialized and running.");
          return true; // Indicate success
        }

        try {
            this.log("Initializing audio pipeline (AudioContext, Analyser)...");
            this.audioContext = new (window.AudioContext || window.webkitAudioContext)();

            this.audioContext.onstatechange = () => {
                this.log(`AudioContext state changed to: ${this.audioContext.state}`);
                if (this.audioContext.state === 'running' && this.vadEnabled && !this.animationId) {
                    this.log("AudioContext resumed, restarting visualization and VAD loop if VAD is enabled.");
                    this.startAudioVisualizationAndVADLoop();
                } else if (this.audioContext.state === 'suspended' || this.audioContext.state === 'closed') {
                    this.error(`AudioContext entered problematic state: ${this.audioContext.state}`);
                    this.stopVADAndVisualization(); // Stop things if context is bad
                }
            };

            if (this.audioContext.state === 'suspended') {
              this.audioContext.resume().catch(e => this.error("Error resuming AudioContext", e));
            }

            this.analyser = this.audioContext.createAnalyser();
            this.analyser.fftSize = 2048;
            this.analyser.smoothingTimeConstant = 0.8; // A common value

            this.sourceNode = this.audioContext.createMediaStreamSource(this.localStream);
            this.sourceNode.connect(this.analyser);
            this.log('üéµ Audio pipeline initialized.');
            return true; // Indicate success
        } catch (e) {
            this.error("Failed to initialize audio pipeline", e);
            this.audioContext = null; // Reset on failure
            return false; // Indicate failure
        }
      }

      startAudioVisualizationAndVADLoop() {
        if (this.animationId) {
            this.log("Visualization loop attempt while already running.");
            return;
        }
        if (!this.analyser || !this.audioContext || this.audioContext.state !== 'running') {
            this.log("Cannot start visualization: Analyser or AudioContext not ready/running.");
            if (!this.initAudioPipeline()) { // if initAudioPipeline returns false (failure)
                this.error("Failed to start visualization: Audio pipeline init failed.");
                this.stopVAD(true); // Turn off VAD if we can't visualize/process
                return;
            }
        }

        this.log("Starting audio visualization and VAD loop.");
        const canvas = this.audioVisualizer;
        const ctx = canvas.getContext('2d');
        if (!ctx) { this.error("Failed to get canvas context for visualizer."); return; }

        canvas.width = canvas.offsetWidth > 0 ? canvas.offsetWidth : 300; // Default if hidden
        canvas.height = canvas.offsetHeight > 0 ? canvas.offsetHeight : 100; // Default if hidden

        const bufferLength = this.analyser.frequencyBinCount;
        const dataArray = new Uint8Array(bufferLength);

        const drawAndProcess = () => {
          if (!this.vadEnabled) { // Check at the beginning of the frame
            if (this.animationId) { // Ensure it's only cancelled once
                cancelAnimationFrame(this.animationId);
                this.animationId = null;
                this.log("VAD disabled, stopping visualization loop.");
                ctx.fillStyle = '#e9ecef'; // Clear canvas
                ctx.fillRect(0, 0, canvas.width, canvas.height);
            }
            return;
          }
          if (!this.analyser || !this.audioContext || this.audioContext.state !== 'running') { // Robustness check
             if (this.animationId) cancelAnimationFrame(this.animationId);
             this.animationId = null;
             this.error("Visualizer stopping: Audio pipeline issue.");
             this.stopVAD(true); // Stop VAD if audio pipeline fails
             return;
          }

          this.animationId = requestAnimationFrame(drawAndProcess); // Request next frame first

          this.analyser.getByteTimeDomainData(dataArray);

          ctx.fillStyle = '#e9ecef';
          ctx.fillRect(0, 0, canvas.width, canvas.height);
          ctx.lineWidth = 2;

          let isAISpeaking = (this.currentServerAudio && !this.currentServerAudio.paused) || (this.synthesis && this.synthesis.speaking);

          if (this.userIsSpeakingVAD) ctx.strokeStyle = '#ffc107'; // Yellow for user speaking
          else if (isAISpeaking) ctx.strokeStyle = '#e83e8c';      // Pink/Purple for AI speaking
          else if (this.vadEnabled) ctx.strokeStyle = '#007bff';   // Blue for VAD active listening
          else ctx.strokeStyle = '#6c757d';                       // Grey for idle/VAD off

          ctx.beginPath();
          const sliceWidth = canvas.width * 1.0 / bufferLength;
          let x = 0;
          for (let i = 0; i < bufferLength; i++) {
            const v = dataArray[i] / 128.0; // dataArray values are 0-255
            const y = v * canvas.height / 2;
            if (i === 0) ctx.moveTo(x, y);
            else ctx.lineTo(x, y);
            x += sliceWidth;
          }
          ctx.lineTo(canvas.width, canvas.height / 2);
          ctx.stroke();

          // VAD Logic
          if (this.vadEnabled && !this.isProcessingAI && !isAISpeaking && !this.isAISpeechCoolDown) {
            if (!this.userIsSpeakingVAD && !this.isCapturingAudio) {
              this.vad_detectSpeechStart(dataArray);
            } else if (this.userIsSpeakingVAD && this.isCapturingAudio) {
              this.vad_detectSpeechContinuation(dataArray);
            }
          }
        };
        drawAndProcess();
      }

      stopVADAndVisualization() {
        this.log("Stopping VAD and visualization (e.g., on disconnect).");
        if (this.vadEnabled) {
            this.stopVAD(false); // Pass false to not update status to "idle" if it's a disconnect
        } else if (this.animationId) { // If VAD was already off but loop somehow running
            cancelAnimationFrame(this.animationId);
            this.animationId = null;
            const canvas = this.audioVisualizer;
            if (canvas) {
                const ctx = canvas.getContext('2d');
                if (ctx) {
                    ctx.fillStyle = '#e9ecef';
                    ctx.fillRect(0, 0, canvas.width, canvas.height);
                }
            }
            this.log("Visualization loop stopped explicitly (VAD was already off).");
        }
        // Disconnect nodes to allow garbage collection and stop processing
        if (this.sourceNode) {
            try { this.sourceNode.disconnect(); } catch(e) { /* ignore */ }
            this.sourceNode = null;
        }
        if (this.analyser) {
            try { this.analyser.disconnect(); } catch(e) { /* ignore */ }
            this.analyser = null;
        }
      }

      async toggleVAD() {
        this.log(`Toggling VAD. Current vadEnabled: ${this.vadEnabled}`);
        if (!this.localStream || !this.localStream.active) {
            this.log("Mic stream not available for VAD. Requesting...");
            await this.requestMicrophoneAccess();
            if (!this.localStream || !this.localStream.active) { // Check again
                this.error("Failed to acquire audio stream for VAD toggle.");
                this.updateStatus('error', 'Mic Error', 'Microphone not available.');
                return;
            }
        }
        if(!this.audioContext || this.audioContext.state !== 'running') {
            this.log("AudioContext not running, (re-)initializing pipeline for VAD toggle.");
            if (!this.initAudioPipeline()) { // initAudioPipeline now returns boolean
                this.error("Failed to initialize audio pipeline for VAD toggle.");
                this.updateStatus('error', 'Pipeline Error', 'Audio system failed.');
                return;
             }
        }

        if (this.vadEnabled) {
          this.stopVAD(true); // Explicitly update to idle status
        } else {
          this.startVAD();
        }
      }

      startVAD() {
        if (this.vadEnabled) { this.log("VAD already active."); return; }
        // Ensure audio pipeline is ready before enabling VAD
        if (!this.audioContext || this.audioContext.state !== 'running') {
            if (!this.initAudioPipeline()) {
                this.error("Cannot start VAD: Audio pipeline failed to initialize.");
                this.updateStatus('error', 'Audio Error', 'Failed to start audio system.');
                return;
            }
        }

        this.log('üü¢ Activating VAD (Hands-Free Mode)...');
        this.vadEnabled = true; // Set this before starting loop
        this.userIsSpeakingVAD = false;
        this.isCapturingAudio = false;
        this.isAISpeechCoolDown = false;
        this.audioChunks = [];
        this.updateStatus('vad-active', 'Listening...', 'Speak when ready.');
        this.voiceBtn.classList.add('vad-listening-mode');
        this.voiceBtn.classList.remove('user-speaking', 'ai-processing', 'ai-speaking', 'status-idle');
        this.voiceBtn.innerHTML = 'üëÇ';
        this.voiceBtnHelpText.textContent = "Listening for your voice...";
        this.transcriptionDisplay.innerHTML = 'Your speech will appear here...';

        if (!this.animationId) { // Only start if not already running
            this.startAudioVisualizationAndVADLoop();
        } else {
            this.log("Visualization loop already running, VAD enabled.");
        }
      }

      stopVAD(updateToIdle = true) {
        if (!this.vadEnabled) { this.log("VAD already inactive."); return; }
        this.log(`üî¥ Deactivating VAD mode. Update to idle: ${updateToIdle}`);
        this.vadEnabled = false; // This will cause the animation loop to stop itself

        if (this.silenceTimeoutId) { clearTimeout(this.silenceTimeoutId); this.silenceTimeoutId = null; }
        this.userIsSpeakingVAD = false;

        if (this.isCapturingAudio) {
          this.log("VAD stopped during active capture. Discarding audio.", "color: orange;");
          this.isCapturingAudio = false; // Signal to onstop to discard
          if (this.mediaRecorder && this.mediaRecorder.state === "recording") {
            this.mediaRecorder.stop(); // onstop will handle discarding if isCapturingAudio is false
          }
        }

        if (updateToIdle) {
            this.updateStatus('idle', 'Ready', 'Click the microphone to start listening');
        }
        this.voiceBtn.classList.remove('vad-listening-mode', 'user-speaking', 'ai-processing', 'ai-speaking');
        this.voiceBtn.classList.add('status-idle'); // Ensure idle class if VAD off
        this.voiceBtn.innerHTML = 'üé§';
        this.voiceBtnHelpText.textContent = "Click to enable hands-free mode";
      }

      calculateAverageEnergy(dataArray) {
        let sum = 0;
        for (let i = 0; i < dataArray.length; i++) {
            const s = (dataArray[i] / 128.0) - 1.0; sum += s * s;
        }
        return Math.sqrt(sum / dataArray.length);
      }

      vad_detectSpeechStart(dataArray) {
          if (this.calculateAverageEnergy(dataArray) > this.VAD_ENERGY_THRESHOLD) {
              this.userIsSpeakingVAD = true;
              this.speechStartTime = Date.now();
              this.updateStatus('listening', 'User Speaking...', 'Capturing your voice');
              this.voiceBtn.classList.add('user-speaking');
              this.voiceBtn.classList.remove('vad-listening-mode', 'status-idle');
              this.transcriptionDisplay.innerHTML = '<strong>Transcription:</strong> Listening<span class="loading-dots"></span>';
              this.startMediaRecorderForCapture();
              if (this.silenceTimeoutId) clearTimeout(this.silenceTimeoutId);
              this.silenceTimeoutId = setTimeout(() => {
                  if (this.userIsSpeakingVAD && this.vadEnabled && this.isCapturingAudio) this.vad_handleEndOfSpeech();
              }, this.VAD_SILENCE_DURATION_MS);
          }
      }

      vad_detectSpeechContinuation(dataArray) {
          if (this.calculateAverageEnergy(dataArray) > this.VAD_ENERGY_THRESHOLD) {
              if (this.silenceTimeoutId) clearTimeout(this.silenceTimeoutId);
              this.silenceTimeoutId = setTimeout(() => {
                  if (this.userIsSpeakingVAD && this.vadEnabled && this.isCapturingAudio) this.vad_handleEndOfSpeech();
              }, this.VAD_SILENCE_DURATION_MS);
          }
      }

      vad_handleEndOfSpeech() {
        if (!this.vadEnabled || (!this.userIsSpeakingVAD && !this.isCapturingAudio)) {
            this.log("vad_handleEndOfSpeech: exiting, VAD disabled or not in relevant state.");
            this.resetToListeningState();
            return;
        }
        if (this.silenceTimeoutId) { clearTimeout(this.silenceTimeoutId); this.silenceTimeoutId = null; }

        const speechDuration = this.speechStartTime ? (Date.now() - this.speechStartTime) : 0;
        this.userIsSpeakingVAD = false; // User is no longer actively speaking according to VAD
        this.log(`üé§ VAD determined End of Speech. Duration: ${speechDuration}ms`);

        if (!this.isCapturingAudio) {
            this.log("Not in capturing audio state when VAD ended speech, likely already handled (e.g. stopped by other means).");
            this.resetToListeningState();
            return;
        }

        if (speechDuration < this.VAD_MIN_SPEECH_DURATION_MS) {
            this.log(`üé§ Speech too short (${speechDuration}ms). Discarding.`, "color: orange;");
            this.isCapturingAudio = false; // Signal to onstop to discard
            if (this.mediaRecorder && this.mediaRecorder.state === "recording") this.mediaRecorder.stop();
            this.transcriptionDisplay.innerHTML = 'Speech too short... Try speaking for a bit longer.';
            this.resetToListeningState();
            return;
        }

        // If speech is long enough, stop the recorder to process.
        if (this.mediaRecorder && this.mediaRecorder.state === "recording") {
            this.log("Valid speech duration, stopping MediaRecorder for processing.");
            this.mediaRecorder.stop(); // onstop will call processCapturedAudio if isCapturingAudio is still true
        } else if (this.isCapturingAudio && this.audioChunks.length > 0) {
            this.log("MediaRecorder was not recording but chunks exist and capturing was intended. Processing.", "color: orange;");
            this.processCapturedAudio(); // Process directly if recorder wasn't active but we have chunks
        } else {
            this.log("No active recorder or chunks to process after VAD end of speech.", "color: orange;");
            this.resetToListeningState();
        }
      }

      startMediaRecorderForCapture() {
        if (!this.localStream || !this.localStream.active) {
            this.error("No local stream for MediaRecorder. Cannot start capture.");
            this.resetToListeningState();
            return;
        }
        if (this.mediaRecorder && this.mediaRecorder.state === "recording") {
            this.log("MediaRecorder already recording.");
            return;
        }
        this.isCapturingAudio = true; // Explicitly set intent to capture
        this.audioChunks = [];
        try {
            this.mediaRecorder = new MediaRecorder(this.localStream, { mimeType: 'audio/webm;codecs=opus' });
            this.mediaRecorder.ondataavailable = (e) => {
                if (e.data.size > 0) this.audioChunks.push(e.data);
            };
            this.mediaRecorder.onstop = () => {
                this.log(`MediaRecorder onstop. isCapturingAudio: ${this.isCapturingAudio}, Chunks: ${this.audioChunks.length}`);
                if (this.audioChunks.length > 0 && this.isCapturingAudio) {
                    this.processCapturedAudio();
                } else {
                    this.audioChunks = [];
                    this.log("MediaRecorder stopped. Discarding chunks or no capture intended.");
                    if (this.vadEnabled && !this.isProcessingAI) this.resetToListeningState(); // Reset to listening if VAD still on and not processing
                }
                // Ensure isCapturingAudio is false after onstop, regardless of processing
                this.isCapturingAudio = false;
            };
            this.mediaRecorder.onerror = (e) => {
                this.error(`MediaRecorder error: ${e.error ? e.error.name : 'Unknown error'}`, e.error);
                this.isCapturingAudio = false;
                this.resetToListeningState();
            };
            this.mediaRecorder.start(1000); // Optional: timeslice to get data more frequently if needed
            this.log("MediaRecorder started.");
        } catch (err) {
            this.error(`MediaRecorder start error: ${err.message}`, err);
            this.isCapturingAudio = false;
            this.resetToListeningState();
        }
      }

      async processCapturedAudio() {
        if (this.audioChunks.length === 0) {
            this.log("No audio chunks to process.", "color: orange;");
            this.isCapturingAudio = false; // Should be false already but ensure
            this.resetToListeningState();
            return;
        }
        if (this.isProcessingAI) {
            this.log("Already processing AI request. Ignoring new one.", "color: orange;");
            this.isCapturingAudio = false; // Reset capture flag
            this.audioChunks = []; // Discard new chunks
            return;
        }

        this.isProcessingAI = true;
        this.updateStatus('processing', 'Processing...', 'AI is analyzing your speech');
        this.voiceBtn.classList.add('ai-processing');
        this.voiceBtn.classList.remove('user-speaking', 'vad-listening-mode', 'status-idle');

        const audioBlob = new Blob(this.audioChunks, { type: 'audio/webm;codecs=opus' });
        this.audioChunks = []; // Clear chunks after creating blob

        const selectedLanguage = this.inputLanguageSelect.value;
        this.log(`Selected input language for STT: ${selectedLanguage}`);

        try {
            const reader = new FileReader();
            reader.onloadend = () => {
                this.log(`Sending audio to server. Size: ${audioBlob.size} bytes`);
                this.socket.emit('audio_data', {
                    audio: reader.result,
                    timestamp: Date.now(),
                    target_lang: selectedLanguage
                });
            };
            reader.readAsDataURL(audioBlob);
        } catch (e) {
            this.error("Error processing captured audio for sending", e);
            this.isProcessingAI = false;
            this.resetToListeningState();
        }
      }

      handleAudioResponse(data) {
        this.isProcessingAI = false;
        this.voiceBtn.classList.remove('ai-processing');

        if (data.success) {
          let transcriptionHTML = ``;
          if (data.transcription) {
            transcriptionHTML += `<span class="detected-lang">(${data.detected_language || data.target_lang || 'N/A'})</span> ${data.transcription}`;
            this.addMessage(data.transcription, 'user');
          } else {
            transcriptionHTML = 'Speech processed (no transcription text).';
            if (data.info) {
                transcriptionHTML = data.info;
            }
          }
          this.transcriptionDisplay.innerHTML = transcriptionHTML;

          const aiResponseText = data.response_text || data.response;
          if (aiResponseText) {
            this.addMessage(aiResponseText, 'assistant');
            if (data.response_audio_base64) {
              this.log("Playing server-synthesized audio.");
              this.playServerAudio(data.response_audio_base64);
            } else {
              this.log("No server audio. Attempting fallback browser TTS.");
              if (data.tts_error) this.log(`Server TTS Note: ${data.tts_error}`, "color: orange;");
              this.speakTextBrowserFallback(aiResponseText, data.response_language);
            }
          } else {
            this.log("Successful processing but no AI text response.");
            this.resetToListeningState();
          }
        } else {
          this.error(`AI Processing failed: ${data.error || 'Unknown error'}`);
          this.transcriptionDisplay.innerHTML = `<span style="color:red;">Error: ${data.error || 'Processing failed'}</span>`;
          this.addMessage(`Error: ${data.error || 'Processing failed'}`, 'assistant');
          this.updateStatus('error', 'AI Error', data.error || 'Processing failed');
          this.startAISpeechCooldown();
        }
      }

      // --- START OF FOCUSED CHANGES FOR STOP SPEAKING ---
      stopSpeaking() {
        let wasSpeakingServer = false;
        let wasSpeakingBrowser = false;
        this.log("Stop speaking button clicked.");

        if (this.currentServerAudio && !this.currentServerAudio.paused) {
          this.log("Attempting to stop server audio playback.");
          try {
            this.currentServerAudio.pause();
            this.currentServerAudio.currentTime = 0; // Optional: Reset playback position
            // Detach event listeners to prevent them from firing after manual stop
            this.currentServerAudio.onplay = null;
            this.currentServerAudio.onended = null;
            this.currentServerAudio.onerror = null;
            this.currentServerAudio.src = ""; // Release resource
            this.log("Server audio paused and resources released.");
          } catch (e) {
            this.error("Error during server audio stop:", e);
          }
          this.currentServerAudio = null; // Crucial: clear the reference
          wasSpeakingServer = true;
        } else {
            this.log("No server audio was playing or currentServerAudio is null.");
        }

        if (this.synthesis && this.synthesis.speaking) {
          this.log("Attempting to stop browser AI speech.");
          try {
            if (this.currentUtterance) {
                this.currentUtterance.onstart = null;
                this.currentUtterance.onend = null;
                this.currentUtterance.onerror = null;
                this.log("Browser utterance event listeners detached (or attempted).");
            }
            this.synthesis.cancel();
            this.log("Browser TTS cancelled.");
          } catch (e) {
            this.error("Error during browser TTS cancel:", e);
          }
          this.currentUtterance = null; // Crucial: clear the reference
          wasSpeakingBrowser = true;
        } else {
            this.log("No browser TTS was speaking or synthesis is not active.");
        }

        this.voiceBtn.classList.remove('ai-speaking');
        this.log("UI 'ai-speaking' class removed.");

        if ((wasSpeakingServer || wasSpeakingBrowser)) {
            if (!this.isAISpeechCoolDown) {
                this.log("Manually triggered AI speech cooldown after stop action.");
                this.startAISpeechCooldown();
            } else {
                this.log("Cooldown already active, just ensuring UI is reset.");
                if (!this.vadEnabled) {
                    this.updateStatus('idle', 'Ready', 'Click the microphone to start listening');
                    this.voiceBtn.classList.remove('vad-listening-mode');
                    this.voiceBtn.classList.add('status-idle');
                }
            }
        } else {
            this.log("Nothing was speaking. Ensuring UI consistency.");
            this.resetToListeningState();
        }
      }

      playServerAudio(audioBase64) {
        if (this.currentServerAudio && !this.currentServerAudio.paused) {
            this.log("Stopping previous server audio before playing new one.");
            this.currentServerAudio.pause();
            this.currentServerAudio.onplay = null;
            this.currentServerAudio.onended = null;
            this.currentServerAudio.onerror = null;
            this.currentServerAudio.src = "";
            this.currentServerAudio = null;
        }
        if (this.synthesis && this.synthesis.speaking) {
            this.log("Cancelling browser TTS before playing server audio.");
            if (this.currentUtterance) {
                this.currentUtterance.onstart = null;
                this.currentUtterance.onend = null;
                this.currentUtterance.onerror = null;
            }
            this.synthesis.cancel();
            this.currentUtterance = null;
        }

        const audio = new Audio(audioBase64);
        this.currentServerAudio = audio;

        this.log(`Setting up new server audio playback. Src: ${audioBase64.substring(0,50)}...`);

        audio.onplay = () => {
          if (audio !== this.currentServerAudio) {
            this.log("Server audio onplay: Stale event, currentServerAudio changed. Ignoring.");
            return;
          }
          this.log("üëÑ AI Server Audio Started...");
          this.updateStatus('speaking', 'AI Speaking...', 'AI is responding');
          this.voiceBtn.classList.add('ai-speaking');
          this.voiceBtn.classList.remove('vad-listening-mode', 'user-speaking', 'ai-processing', 'status-idle');
        };

        audio.onended = () => {
          if (audio !== this.currentServerAudio && this.currentServerAudio !== null) {
            this.log("Server audio onended: Stale event or already handled by stopSpeaking. Ignoring.");
            return;
          }
          this.log("üëÑ AI Server Audio Finished (onended).");
          if (this.currentServerAudio === audio) {
              this.currentServerAudio = null;
          }
          if (!this.isAISpeechCoolDown) this.startAISpeechCooldown(); else this.log("Cooldown already active on server audio end.");
        };

        audio.onerror = (e) => {
          if (audio !== this.currentServerAudio && this.currentServerAudio !== null) {
            this.log("Server audio onerror: Stale event or already handled by stopSpeaking. Ignoring.");
            return;
          }
          this.error("Error playing server audio (onerror)", e);
          if (this.currentServerAudio === audio) {
              this.currentServerAudio = null;
          }
          if (!this.isAISpeechCoolDown) this.startAISpeechCooldown(); else this.log("Cooldown already active on server audio error.");
        };

        audio.play().catch(e => {
          if (audio !== this.currentServerAudio && this.currentServerAudio !== null) {
            this.log("Server audio play().catch: Stale event or already handled by stopSpeaking. Ignoring.");
            return;
          }
          this.error("Audio play() promise rejected for server audio", e);
          if (this.currentServerAudio === audio) {
            this.currentServerAudio = null;
          }
          if (!this.isAISpeechCoolDown) this.startAISpeechCooldown(); else this.log("Cooldown already active on server audio play catch.");
        });
      }

      speakTextBrowserFallback(text, lang) {
        if (this.currentServerAudio && !this.currentServerAudio.paused) {
             this.log("Stopping server audio to play browser TTS.");
             this.currentServerAudio.pause();
             this.currentServerAudio.onplay = null;
             this.currentServerAudio.onended = null;
             this.currentServerAudio.onerror = null;
             this.currentServerAudio.src = "";
             this.currentServerAudio = null;
        }
        if (this.synthesis && this.synthesis.speaking) {
            this.log("Cancelling previous browser TTS before speaking new one.");
            if (this.currentUtterance) {
                this.currentUtterance.onstart = null;
                this.currentUtterance.onend = null;
                this.currentUtterance.onerror = null;
            }
            this.synthesis.cancel();
            this.currentUtterance = null;
        }

        const utterance = new SpeechSynthesisUtterance(text);
        this.currentUtterance = utterance;

        const voices = this.synthesis.getVoices();
        let selectedVoice = null;
        if (lang && voices && voices.length > 0) {
            utterance.lang = lang;
            const primaryLang = lang.split('-')[0].toLowerCase();
            selectedVoice = voices.find(v => v.lang.toLowerCase() === lang.toLowerCase() && v.default) ||
                            voices.find(v => v.lang.toLowerCase() === lang.toLowerCase()) ||
                            voices.find(v => v.lang.toLowerCase().startsWith(primaryLang + "-") && v.default) ||
                            voices.find(v => v.lang.toLowerCase().startsWith(primaryLang + "-")) ||
                            voices.find(v => v.lang.toLowerCase() === primaryLang && v.default) ||
                            voices.find(v => v.lang.toLowerCase() === primaryLang);
        }
        if (selectedVoice) {
            utterance.voice = selectedVoice;
            this.log(`Using browser TTS voice: ${selectedVoice.name} (${selectedVoice.lang}) for lang: ${lang}`);
        } else {
            const preferredVoiceIndex = this.voiceSelect.value;
            if (preferredVoiceIndex && voices && voices[preferredVoiceIndex]) {
                utterance.voice = voices[preferredVoiceIndex];
                this.log(`Using preferred fallback TTS voice from dropdown: ${utterance.voice.name}`);
            } else {
                this.log(`No specific browser TTS voice for "${lang}". Using browser default.`);
            }
        }
        utterance.rate = parseFloat(this.speedRange.value);
        utterance.pitch = parseFloat(this.pitchRange.value);

        utterance.onstart = () => {
          if (utterance !== this.currentUtterance) {
            this.log("Browser TTS onstart: Stale event. Ignoring.");
            return;
          }
          this.log("üëÑ AI Browser TTS Started...");
          this.updateStatus('speaking', 'AI Speaking...', 'AI is responding');
          this.voiceBtn.classList.add('ai-speaking');
          this.voiceBtn.classList.remove('vad-listening-mode', 'user-speaking', 'ai-processing', 'status-idle');
        };

        utterance.onend = () => {
          if ((utterance !== this.currentUtterance && this.currentUtterance !== null) || !this.synthesis.speaking && this.currentUtterance === null) {
            this.log("Browser TTS onend: Stale event or already handled by stopSpeaking/cancel. Ignoring.");
            return;
          }
          this.log("üëÑ AI Browser TTS Finished (onend).");
          if (this.currentUtterance === utterance) {
              this.currentUtterance = null;
          }
          if (!this.isAISpeechCoolDown) this.startAISpeechCooldown(); else this.log("Cooldown already active on browser TTS end.");
        };

        utterance.onerror = (e) => {
          if ((utterance !== this.currentUtterance && this.currentUtterance !== null) || !this.synthesis.speaking && this.currentUtterance === null) {
            this.log("Browser TTS onerror: Stale event or already handled by stopSpeaking/cancel. Ignoring.");
            return;
          }
          this.error(`Browser TTS error (onerror): ${e.error}`, e);
          if (this.currentUtterance === utterance) {
              this.currentUtterance = null;
          }
          if (!this.isAISpeechCoolDown) this.startAISpeechCooldown(); else this.log("Cooldown already active on browser TTS error.");
        };

        try {
            this.synthesis.speak(utterance);
        } catch (e) {
            this.error("Error calling speechSynthesis.speak", e);
            if (this.currentUtterance === utterance) {
                this.currentUtterance = null;
            }
            if (!this.isAISpeechCoolDown) this.startAISpeechCooldown();
        }
      }

      startAISpeechCooldown() {
        if (this.isAISpeechCoolDown) {
            this.log("Cooldown already in progress.");
            return;
        }
        this.isAISpeechCoolDown = true;
        this.voiceBtn.classList.remove('ai-speaking');
        this.log(`AI speech cool-down started (${this.AI_SPEECH_COOLDOWN_MS}ms).`);

        if (this.cooldownTimeoutId) clearTimeout(this.cooldownTimeoutId);

        this.cooldownTimeoutId = setTimeout(() => {
          this.isAISpeechCoolDown = false;
          this.cooldownTimeoutId = null;
          this.log("AI speech cool-down finished.");
          this.resetToListeningState();
        }, this.AI_SPEECH_COOLDOWN_MS);
      }
      // --- END OF FOCUSED CHANGES FOR STOP SPEAKING ---


      resetToListeningState() {
          // Only reset to listening if VAD is enabled and no other conflicting states are active
          if (this.vadEnabled &&
              !this.isProcessingAI &&
              !this.userIsSpeakingVAD &&
              (!this.currentServerAudio || this.currentServerAudio.paused) && // Server audio not playing
              (!this.synthesis || !this.synthesis.speaking) && // Browser TTS not speaking
              !this.isAISpeechCoolDown) { // Not in cooldown
            this.updateStatus('vad-active', 'Listening...', 'Speak when ready.');
            this.voiceBtn.classList.add('vad-listening-mode');
            this.voiceBtn.classList.remove('ai-speaking', 'user-speaking', 'ai-processing', 'status-idle');
          } else if (!this.vadEnabled) { // If VAD is explicitly off, go to idle
            this.updateStatus('idle', 'Ready', 'Click the microphone to start listening');
            this.voiceBtn.classList.remove('ai-speaking', 'user-speaking', 'ai-processing', 'vad-listening-mode');
            this.voiceBtn.classList.add('status-idle');
          } else {
            this.log("Conditions not met to reset to VAD listening state. Current state preserved.");
            // This might happen if, e.g., VAD is enabled but AI is still processing or in cooldown.
          }
      }

      addMessage(text, sender) {
        const messageBubble = document.createElement('div');
        messageBubble.className = `message-bubble message-${sender}`;
        messageBubble.textContent = text;

        const wrapper = document.createElement('div');
        wrapper.style.display = 'flex';
        wrapper.style.width = '100%';
        if (sender === 'user') {
            wrapper.style.justifyContent = 'flex-end';
        } else {
            wrapper.style.justifyContent = 'flex-start';
        }
        wrapper.appendChild(messageBubble);
        this.chatContainer.appendChild(wrapper);
        this.chatContainer.scrollTop = this.chatContainer.scrollHeight;
      }

      bindEvents() {
        this.voiceBtn.addEventListener('click', () => this.toggleVAD());
        this.clearChat.addEventListener('click', () => { this.log("Clearing chat."); this.socket.emit('clear_conversation'); });
        this.stopSpeaking.addEventListener('click', () => this.stopSpeaking()); // This calls the modified stopSpeaking
        this.testGemini.addEventListener('click', () => this.testGeminiAPI());
        this.speedRange.addEventListener('input', (e) => { this.speedValue.textContent = e.target.value; });
        this.pitchRange.addEventListener('input', (e) => { this.pitchValue.textContent = e.target.value; });
        this.log("UI Events Bound");
      }

      testGeminiAPI() {
        this.log("Testing Gemini API via server...");
        if (this.vadEnabled) { this.stopVAD(true); } // Stop VAD if active, update to idle

        this.updateStatus('processing', 'Testing API...', 'Sending test request.');
        this.voiceBtn.classList.add('ai-processing');

        const testLang = this.inputLanguageSelect ? this.inputLanguageSelect.value : 'en';
        this.log(`Test Gemini using language context: ${testLang}`);

        fetch(`/test-gemini?lang=${testLang}`)
          .then(response => {
            if (!response.ok) { throw new Error(`HTTP error! status: ${response.status}, message: ${response.statusText}`); }
            return response.json();
          })
          .then(data => {
            this.isProcessingAI = false;
            this.voiceBtn.classList.remove('ai-processing');

            if (data.success && data.response_text) {
              this.log("Gemini API Test Successful: " + data.response_text);
              this.addMessage(`Gemini API Test (Manual - Lang: ${data.response_language || testLang})`, 'user');
              this.addMessage(data.response_text, 'assistant');
              if (data.response_audio_base64) {
                this.playServerAudio(data.response_audio_base64);
              } else {
                this.speakTextBrowserFallback(data.response_text, data.response_language || 'en');
              }
            } else {
              this.error('Gemini API Test Failed or invalid response structure', data.error || data);
              this.addMessage('Gemini API Test Failed: ' + (data.error || 'Unknown error from server'), 'assistant');
              this.resetToListeningState(); // Reset on error
            }
          })
          .catch(err => {
            this.isProcessingAI = false; // Reset flag after fetch error
            this.voiceBtn.classList.remove('ai-processing');
            this.error('Failed to fetch /test-gemini', err);
            this.addMessage('Failed to test Gemini API (network/fetch error)', 'assistant');
            this.resetToListeningState(); // Reset on error
          });
      }

      updateStatus(statusKey, text, detail = "") {
        if (this.statusIndicator) this.statusIndicator.className = `status-indicator status-${statusKey}`;
        if (this.statusText) this.statusText.textContent = text;
        if (this.statusDetail) this.statusDetail.textContent = detail;
      }
    }

    document.addEventListener('DOMContentLoaded', () => {
      try {
        window.voiceApp = new VoiceAI();
      } catch (e) {
        console.error("Fatal error initializing VoiceAI:", e);
        const body = document.body;
        if (body) { // Check if body exists
            const errorDiv = document.createElement('div');
            errorDiv.style.cssText = 'position:fixed;top:0;left:0;width:100%;background-color:red;color:white;padding:20px;z-index:9999;text-align:center;';
            errorDiv.innerHTML = '<h2>Application Error</h2><p>Could not initialize the Voice AI. Please check the console (F12) for details and refresh the page. Ensure your browser supports WebRTC and Web Audio.</p>';
            body.prepend(errorDiv);
        }
      }
    });
  </script>
</body>
</html>
